from google.colab import drive
import imageio
import os
from vizdoom import *
import numpy as np
import time
import torch
import torch.nn as nn
import torch.optim as optim
import random
import pandas as pd
from datetime import datetime
import glob
import psutil
import gc
from collections import Counter, namedtuple
import cv2
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter  # Import TensorBoard
import copy # used to deepcopy the target network

# --- Configuration ---
RECORD_LMP = False
RECORD_VIDEO = False
VIDEO_FPS = 30
LMP_DIR = "lmp_recordings"
SCENARIO_NAME = "defend_the_center.cfg"
# --- Google Drive Paths ---
DRIVE_MODEL_DIR = "/content/drive/My Drive/ViZDoomModels"
DRIVE_WAD_DIR = "/content/drive/My Drive/ViZDoomWADs"
# --- Local Paths ---
LOCAL_WAD_DIR = "/content/ViZDoomWADs"
# --- Frame Stacking ---
STACK_SIZE = 4

# --- User Input for Recording, Number of Episodes, and FRAME_SKIP ---
record_choice = input("Do you want to record the game? (yes/no): ").lower()
if record_choice == "yes":
    record_lmp_choice = input("Record .lmp demo files? (yes/no): ").lower()
    RECORD_LMP = record_lmp_choice == "yes"
    record_video_choice = input("Record video (MP4)? (yes/no): ").lower()
    RECORD_VIDEO = record_video_choice == "yes"

num_episodes_choice = input("Enter the number of episodes to run: ")
try:
    NUM_EPISODES = int(num_episodes_choice)
except ValueError:
    print("Invalid input. Using default number of episodes (10).")
    NUM_EPISODES = 10

# User input for FRAME_SKIP during training and recording
FRAME_SKIP_TRAINING = int(input("Enter FRAME_SKIP value for training (default 4): ") or "4")
FRAME_SKIP_RECORDING = int(input("Enter FRAME_SKIP value for recording (default 1): ") or "1")
FRAME_SKIP = FRAME_SKIP_TRAINING if not RECORD_VIDEO else FRAME_SKIP_RECORDING

# --- Google Drive Setup ---
drive_mounted = os.path.exists('/content/drive/My Drive')
if not drive_mounted:
    drive.mount('/content/drive')

VIDEO_DIR = "/content/drive/My Drive/ViZDoomRecordings"
VIDEO_FILENAME = "game_recording.mp4"
VIDEO_PATH = os.path.join(VIDEO_DIR, VIDEO_FILENAME)
os.makedirs(VIDEO_DIR, exist_ok=True)
os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)
os.makedirs(DRIVE_WAD_DIR, exist_ok=True)
os.makedirs(LOCAL_WAD_DIR, exist_ok=True)

# --- Ask user for WAD choice ---
print("Choose a WAD file to use:")
print("1. Use current scenario WAD (defend_the_center.cfg with vizdoom assets)")
print("2. Use original Doom WAD from Google Drive (ViZDoomWADs folder with defend_the_center.cfg)")
wad_choice = input("Enter your choice (1 or 2): ")

if wad_choice == "1":
    import vizdoom
    vizdoom_path = os.path.dirname(vizdoom.__file__)
    SCENARIO_PATH = os.path.join(vizdoom_path, "scenarios", SCENARIO_NAME)
    wad_path = None

elif wad_choice == "2":
    wad_files = glob.glob(os.path.join(DRIVE_WAD_DIR, "*.wad"))
    if wad_files:
        print("Available WAD files in ViZDoomWADs:")
        for i, file in enumerate(wad_files):
            print(f"{i+1}. {os.path.basename(file)}")
        wad_file_choice = input("Enter the number of the WAD file to use: ")
        try:
            wad_file_choice = int(wad_file_choice)
            if 1 <= wad_file_choice <= len(wad_files):
                wad_path = wad_files[wad_file_choice - 1]
                local_wad_path = os.path.join(LOCAL_WAD_DIR, os.path.basename(wad_path))
                if not os.path.exists(local_wad_path):
                    print(f"Copying {os.path.basename(wad_path)} from Google Drive to local...")
                    import shutil
                    shutil.copy(wad_path, local_wad_path)
                    print("Copy complete.")
                else:
                    print(f"{os.path.basename(wad_path)} already exists locally. Using local copy.")
                wad_path = local_wad_path
                SCENARIO_NAME = "defend_the_center.cfg"
                import vizdoom
                vizdoom_path = os.path.dirname(vizdoom.__file__)
                SCENARIO_PATH = os.path.join(vizdoom_path, "scenarios", SCENARIO_NAME)

            else:
                raise ValueError("Invalid WAD file choice.")
        except ValueError:
            print("Invalid input. Using default scenario.")
            wad_path = None
    else:
        print("No WAD files found in ViZDoomWADs. Using default scenario.")
        wad_path = None
else:
    print("Invalid choice. Using default scenario.")
    wad_path = None


# --- DQN ---
class DQN(nn.Module):
    def __init__(self, input_shape, num_actions):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(self.get_conv_output(input_shape), 512)
        self.fc2 = nn.Linear(512, num_actions)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

    def get_conv_output(self, shape):
        o = self.conv1(torch.zeros(1, *shape))
        o = self.conv2(o)
        o = self.conv3(o)
        return int(np.prod(o.size()))

# --- Prioritized Experience Replay Memory ---
class PrioritizedReplayMemory:
    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):  # Added beta parameters
        self.capacity = capacity
        self.memory = []
        self.position = 0
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        self.frame = 0

    def push(self, state, action, reward, next_state, done):
        max_priority = self.priorities.max() if self.memory else 1.0  # Ensure initial priority is 1.0 if memory is empty

        if len(self.memory) < self.capacity:
            self.memory.append(None)

        self.memory[self.position] = (state, action, reward, next_state, done)
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        priorities = self.priorities[:len(self.memory)]  # Only sample from the current memory
        probabilities = priorities**self.alpha
        probabilities /= probabilities.sum()

        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)
        samples = [self.memory[idx] for idx in indices]

        # Compute IS weights
        beta = min(1.0, self.beta_start + (self.frame * (1 - self.beta_start) / self.beta_frames))
        weights = (len(self.memory) * probabilities[indices])**(-beta)
        weights /= weights.max()
        self.frame += 1

        return samples, indices, weights

    def update_priorities(self, indices, td_errors):
        self.priorities[indices] = np.abs(td_errors) + 1e-6

    def __len__(self):
        return len(self.memory)

# --- N-step Buffer ---
class NStepBuffer:
    def __init__(self, n_step):
        self.n_step = n_step
        self.buffer = []

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def get(self):
        if len(self.buffer) < self.n_step:
            return None

        n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done = self.buffer[0]
        for i in range(1, self.n_step):
            state, action, reward, next_state, done = self.buffer[i]
            n_step_reward += reward * GAMMA**i
            n_step_next_state = next_state
            n_step_done = done or n_step_done

        self.buffer = self.buffer[1:]
        return n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done

    def __len__(self):
        return len(self.buffer)

class DQNAgent:
    def __init__(self, state_shape, num_actions, learning_rate, gamma, epsilon_start, epsilon_end, epsilon_decay, memory_capacity, batch_size, tau, n_step):
        self.state_shape = state_shape
        self.num_actions = num_actions
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.tau = tau
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy_net = DQN(state_shape, num_actions).to(self.device)
        self.target_net = DQN(state_shape, num_actions).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        self.memory = PrioritizedReplayMemory(memory_capacity)
        self.n_step_buffer = NStepBuffer(n_step)


    def select_action(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.choice(self.num_actions)
        else:
            with torch.no_grad():
                state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
                q_values = self.policy_net(state)
                return q_values.argmax(dim=1).item()

    def update_epsilon(self):
        self.epsilon = max(self.epsilon_end, self.epsilon * self.epsilon_decay)

    def learn(self):
      if len(self.memory) < self.batch_size:
          return

      samples, indices, weights = self.memory.sample(self.batch_size)
      batch = tuple(zip(*samples))

      state_batch = torch.tensor(np.array(batch[0]), dtype=torch.float32, device=self.device)
      action_batch = torch.tensor(batch[1], dtype=torch.long, device=self.device).unsqueeze(1)
      reward_batch = torch.tensor(batch[2], dtype=torch.float32, device=self.device).unsqueeze(1)
      next_state_batch = torch.tensor(np.array(batch[3]), dtype=torch.float32, device=self.device)
      done_batch = torch.tensor(batch[4], dtype=torch.float32, device=self.device).unsqueeze(1)
      weights = torch.tensor(weights, dtype=torch.float32, device=self.device).unsqueeze(1)

      q_values = self.policy_net(state_batch).gather(1, action_batch)

      with torch.no_grad():
          next_q_values_online = self.policy_net(next_state_batch) # For Double DQN
          next_actions = next_q_values_online.argmax(dim=1).unsqueeze(1) # For Double DQN
          next_q_values_target = self.target_net(next_state_batch).gather(1, next_actions) # For Double DQN
          expected_q_values = reward_batch + self.gamma * next_q_values_target * (1 - done_batch)


      td_errors = expected_q_values - q_values # For PER
      loss = (weights * nn.MSELoss(reduction='none')(q_values, expected_q_values)).mean() # For PER

      self.optimizer.zero_grad()
      loss.backward()
      # torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), GRAD_CLIP_NORM) #Clip Grad if you would like
      self.optimizer.step()

      # Use squeeze to remove the extra dimension
      self.memory.update_priorities(indices, td_errors.detach().cpu().numpy().squeeze())

    def update_target_network(self):
      for target_param, local_param in zip(self.target_net.parameters(), self.policy_net.parameters()):
            target_param.data.copy_(self.tau*local_param.data + (1.0-self.tau)*target_param.data)


# --- User Input for Training Parameters ---
print("Enter training parameters (press Enter to use default values):")

MODEL_SAVE_FREQ = int(input(f"  Model Save Frequency (episodes between saves, default: {10}): ") or "10")
BATCH_SIZE = int(input(f"  Batch Size (default: {64}): ") or "64")
MEMORY_CAPACITY = int(input(f"  Memory Capacity (default: {10000}): ") or "10000")
LEARNING_RATE = float(input(f"  Learning Rate (default: {0.0001}): ") or "0.0001")
TAU = float(input(f"  Soft Target Update (default 0.005): ") or "0.005") # Soft Target Update
N_STEP = int(input(f"  N-Step Return (default: 3): ") or "3")

# --- Training Parameters ---
GAMMA = 0.99
EPSILON_START = 1.0
EPSILON_END = 0.05
EPSILON_DECAY = 0.995

# --- ViZDoom Setup ---
game = DoomGame()
game.load_config(SCENARIO_PATH)
if wad_path:
    game.set_doom_game_path(wad_path)
game.set_window_visible(False)
game.set_screen_format(ScreenFormat.RGB24)
game.set_screen_resolution(ScreenResolution.RES_320X240)

if RECORD_LMP:
    game.set_mode(Mode.PLAYER)
    os.makedirs(LMP_DIR, exist_ok=True)

game.init()
num_actions = game.get_available_buttons_size()
actions = np.identity(num_actions, dtype=int).tolist()

screen_height, screen_width = game.get_screen_height(), game.get_screen_width()
channels = game.get_screen_channels()
state_shape = (STACK_SIZE * channels, 84, 84)

# --- Scan for existing models ---
model_files = glob.glob(os.path.join(DRIVE_MODEL_DIR, "*.pth"))

if model_files:
    print("Available models:")
    for i, file in enumerate(model_files):
        print(f"{i+1}. {os.path.basename(file)}")
    print(f"{len(model_files)+1}. Create new model")

    choice = input("Enter your choice: ")
    try:
        choice = int(choice)
        if choice in range(1, len(model_files) + 2):
          if choice <= len(model_files):
            model_path = model_files[choice - 1]
            agent = DQNAgent(state_shape, num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP)
            agent.policy_net.load_state_dict(torch.load(model_path, map_location=agent.device))
            agent.target_net.load_state_dict(agent.policy_net.state_dict())
            print(f"Model loaded from {model_path}")
          else:
            agent = DQNAgent(state_shape, num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP)
            print("Creating new model...")
        else:
            print("Invalid choice. Creating new model...")
            agent = DQNAgent(state_shape, num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP)
    except ValueError:
        print("Invalid input. Creating new model...")
        agent = DQNAgent(state_shape, num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP)
else:
    print("No existing models found. Creating new model...")
    agent = DQNAgent(state_shape, num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP)

# --- Video Writer ---
if RECORD_VIDEO:
    writer = imageio.get_writer(VIDEO_PATH, fps=VIDEO_FPS)

# --- TensorBoard Setup ---
now = datetime.now()
dt_string = now.strftime("%Y%m%d%H%M")
log_dir = os.path.join("runs", f"{dt_string}_experiment")
writer = SummaryWriter(log_dir=log_dir)

# --- Training Loop ---
TICRATE = 35

episode_rewards = []
episode_lengths = []
episode_damage_taken = []
episode_damage_inflicted = []
episode_survival_times = []
inference_times = []
training_start_time = time.time()


# --- Frame Stacking ---
def preprocess_frame(frame):
    if len(frame.shape) == 3 and frame.shape[0] == 3:
        frame = np.mean(frame, axis=0)
    frame = frame.astype(np.float32) / 255.0
    frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)
    if len(frame.shape) == 3:
        frame = frame.transpose(2, 0, 1)
    else:
        frame = np.expand_dims(frame, axis=0)
    return frame


def create_stacked_state(state_buffer):
    processed_frames = [preprocess_frame(f) for f in state_buffer]
    if any(f is None for f in processed_frames) or not all(f.shape == processed_frames[0].shape for f in processed_frames):
        print("Error: Inconsistent or None frames in processed_frames")
        return None
    stacked_state = np.concatenate(processed_frames, axis=0)
    return stacked_state


def get_game_state_info(game):
    game_state = game.get_state()
    if game_state is not None:
      damage_taken = game_state.game_variables[0]
      damage_inflicted = game_state.game_variables[1]
      return damage_taken, damage_inflicted
    else:
        return 0, 0


def run_episode(agent, game, actions, episode, frame_skip, lmp_dir, record_video, video_writer, stack_size, state_shape):
    if RECORD_LMP:
      lmp_file_path = os.path.join(lmp_dir, f"episode_{episode + 1}.lmp")
      game.new_episode(lmp_file_path)
    else:
      game.new_episode()

    game_state = game.get_state()
    state_buffer = [game_state.screen_buffer] * stack_size
    state = create_stacked_state(state_buffer)
    if state is None:
        print(f"Skipping episode {episode + 1} due to state initialization error.")
        return None, 0, 0, 0, 0, 0  # Return default values if skipped

    total_reward = 0
    step_count = 0
    episode_start_time = time.time()
    damage_taken = 0
    damage_inflicted = 0
    action_counts = Counter()
    inference_times = []

    while not game.is_episode_finished():
        inference_start_time = time.time()
        action_index = agent.select_action(state)
        inference_end_time = time.time()
        inference_times.append(inference_end_time - inference_start_time)

        action = actions[action_index]
        action_counts[action_index] += 1

        reward = game.make_action(action, frame_skip)
        done = game.is_episode_finished()

        if not done:
            game_state = game.get_state()
            if game_state and game_state.screen_buffer is not None:
                next_frame = game_state.screen_buffer
                state_buffer.pop(0)
                state_buffer.append(next_frame)
                next_state = create_stacked_state(state_buffer)
                if next_state is None:
                  print(f"Error: next_state is None in episode {episode + 1} at step {step_count + 1}.")
                  break
                damage_taken_step, damage_inflicted_step = get_game_state_info(game)
                damage_taken += damage_taken_step
                damage_inflicted += damage_inflicted_step
            else:
              print(f"Error: Invalid state in episode {episode+1}, step {step_count+1}.")
              next_state = np.zeros(state_shape)
              break
        else:
            next_state = np.zeros(state_shape)


        agent.n_step_buffer.push(state, action_index, reward, next_state, done)

        n_step_experience = agent.n_step_buffer.get()
        if n_step_experience:
          n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done = n_step_experience
          agent.memory.push(n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done)


        agent.learn() # Agent learns after n-step experience

        state = next_state
        total_reward += reward
        step_count += 1

        if record_video and not done:
          most_recent_frame = state_buffer[-1]
          video_writer.append_data(most_recent_frame)


    episode_survival_time = time.time() - episode_start_time
    total_actions = sum(action_counts.values())
    if total_actions > 0:
      action_diversity = sum(count / total_actions for count in action_counts.values()) / len(actions)
    else:
      action_diversity = 0

    avg_inference_time = np.mean(inference_times) if inference_times else 0

    return total_reward, step_count, episode_survival_time, damage_taken, damage_inflicted, action_diversity, avg_inference_time


# --- Main Training Loop ---
for episode in range(NUM_EPISODES):
    (
      total_reward,
      step_count,
      episode_survival_time,
      damage_taken,
      damage_inflicted,
      action_diversity,
      avg_inference_time
    ) = run_episode(agent, game, actions, episode, FRAME_SKIP, LMP_DIR, RECORD_VIDEO, writer, STACK_SIZE, state_shape)
    if total_reward is None:
      continue

    episode_survival_times.append(episode_survival_time)
    episode_rewards.append(total_reward)
    episode_lengths.append(step_count)
    episode_damage_taken.append(damage_taken)
    episode_damage_inflicted.append(damage_inflicted)
    agent.update_epsilon()

    if episode % MODEL_SAVE_FREQ == 0:
        model_filename = f"{dt_string}_dqn_model_episode_{episode + 1}.pth"
        model_path = os.path.join(DRIVE_MODEL_DIR, model_filename)
        torch.save(agent.policy_net.state_dict(), model_path)


    agent.update_target_network()

    # --- Get memory usage ---
    process = psutil.Process(os.getpid())
    ram_usage = process.memory_info().rss / 1024**2
    gpu_memory_usage = torch.cuda.max_memory_allocated() / 1024**2 if torch.cuda.is_available() else 0

    # --- Logging ---
    avg_reward = np.mean(episode_rewards[-100:])
    avg_survival_time = np.mean(episode_survival_times[-100:])

    print(
        f"Episode {episode+1}/{NUM_EPISODES}, Total Reward: {total_reward}, Steps: {step_count}, "
        f"Epsilon: {agent.epsilon:.3f}, Avg Reward (last 100): {avg_reward:.2f}, "
        f"Survival Time: {episode_survival_time:.2f}s, Avg Survival Time (last 100): {avg_survival_time:.2f}s, "
        f"Damage Taken: {damage_taken}, Damage Inflicted: {damage_inflicted}, Avg Inference Time: {avg_inference_time:.4f}s, "
        f"RAM Usage: {ram_usage:.2f}MB, GPU Mem Usage: {gpu_memory_usage:.2f}MB, Action Diversity: {action_diversity:.2f}"
    )

    writer.add_scalar("Reward/Total", total_reward, episode)
    writer.add_scalar("Reward/Average", avg_reward, episode)
    writer.add_scalar("Epsilon", agent.epsilon, episode)
    writer.add_scalar("Survival Time/Average", avg_survival_time, episode)
    writer.add_scalar("Action Diversity", action_diversity, episode)
    writer.add_scalar("Damage Taken", damage_taken, episode)
    writer.add_scalar("Damage Inflicted", damage_inflicted, episode)
    writer.add_scalar("RAM Usage", ram_usage, episode)
    writer.add_scalar("GPU Usage", gpu_memory_usage, episode)
    writer.add_scalar("Avg Inference Time", avg_inference_time, episode)
    # Add histograms if you would like.
    # for name, param in agent.policy_net.named_parameters():
    #   writer.add_histogram(name, param, episode)


    # --- Clear memory ---
    gc.collect()
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

# --- Save metrics to CSV ---
training_end_time = time.time()
metrics_df = pd.DataFrame({
    'episode': range(1, NUM_EPISODES + 1),
    'reward': episode_rewards,
    'steps': episode_lengths,
    'epsilon': [EPSILON_START * (EPSILON_DECAY ** i) for i in range(NUM_EPISODES)],
    'training_time': training_end_time - training_start_time,
    'survival_time': episode_survival_times,
    'damage_taken': episode_damage_taken,
    'damage_inflicted': episode_damage_inflicted,
    'ram_usage': ram_usage,
    'gpu_memory_usage': gpu_memory_usage,
    'action_diversity': action_diversity,
    'model_save_freq': MODEL_SAVE_FREQ,
    'batch_size': BATCH_SIZE,
    'memory_capacity': MEMORY_CAPACITY,
    'learning_rate': LEARNING_RATE,
    'tau': TAU,
    'n_step': N_STEP
})

metrics_filename = f"{dt_string}_training_metrics.csv"
metrics_path = os.path.join(DRIVE_MODEL_DIR, metrics_filename)
metrics_df.to_csv(metrics_path, index=False)

# Close the writer
if RECORD_VIDEO:
    writer.close()
writer.close()

game.close()
print("Done!")
