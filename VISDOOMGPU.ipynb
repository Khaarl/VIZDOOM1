import os
import json
import yaml
import shutil
import time
import random
import logging
import glob
import psutil
import gc
from collections import Counter
from datetime import datetime

import cv2
import numpy as np
import pandas as pd
import imageio

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from copy import deepcopy

# --- Create config.yaml if it doesn't exist ---
CONFIG_PATH = "config.yaml"  # Path to your config file

default_config = {
    "drive_model_dir": "/content/drive/My Drive/ViZDoomModels",
    "drive_wad_dir": "/content/drive/My Drive/ViZDoomWADs",
    "local_wad_dir": "/content/ViZDoomWADs",
    "video_dir": "/content/drive/My Drive/ViZDoomRecordings",
    "video_filename": "game_recording.mp4",
    "record_lmp": False,
    "record_video": True,
    "video_fps": 30,
    "lmp_dir": "lmp_recordings",
    "scenario_name": "defend_the_center.cfg",
    "stack_size": 4,
    "num_episodes": 10,
    "frame_skip_training": 4,
    "frame_skip_recording": 1,
    "gamma": 0.99,
    "epsilon_start": 1.0,
    "epsilon_end": 0.05,
    "epsilon_decay": 0.995,
    "model_save_freq": 10,
    "batch_size": 64,
    "memory_capacity": 10000,
    "learning_rate": 0.0001,
    "tau": 0.005,
    "n_step": 3,
    "grad_clip_norm": 1.0,
    "epsilon_decay_rate_step": 1000,
    "use_best_model_callback": True,
    "best_model_smoothing_window": 10,
    "validation_episodes": 5,
}

if not os.path.exists(CONFIG_PATH):
    with open(CONFIG_PATH, 'w') as f:
        yaml.dump(default_config, f)

# --- Configuration Loading ---
def load_config(config_path=CONFIG_PATH):
    with open(config_path, 'r') as file:
        return yaml.safe_load(file)

CONFIG = load_config()

# --- Logging Setup ---
def setup_logger(log_dir, timestamp):
    log_file = os.path.join(log_dir, f"training_{timestamp}.log")
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.DEBUG)

    # Remove existing handlers
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)

    file_handler = logging.FileHandler(log_file)
    file_handler.setLevel(logging.DEBUG)
    stream_handler = logging.StreamHandler()
    stream_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    file_handler.setFormatter(formatter)
    stream_handler.setFormatter(formatter)
    logger.addHandler(file_handler)
    logger.addHandler(stream_handler)
    return logger

# --- Google Drive Setup ---
DRIVE_MODEL_DIR = CONFIG["drive_model_dir"]
DRIVE_WAD_DIR = CONFIG["drive_wad_dir"]
LOCAL_WAD_DIR = CONFIG["local_wad_dir"]
VIDEO_DIR = CONFIG["video_dir"]
VIDEO_FILENAME = CONFIG["video_filename"]
VIDEO_PATH = os.path.join(VIDEO_DIR, VIDEO_FILENAME)

# --- Configuration Variables ---
RECORD_LMP = CONFIG["record_lmp"]
RECORD_VIDEO = CONFIG["record_video"]
VIDEO_FPS = CONFIG["video_fps"]
LMP_DIR = CONFIG["lmp_dir"]
SCENARIO_NAME = CONFIG["scenario_name"]
STACK_SIZE = CONFIG["stack_size"]
NUM_EPISODES = CONFIG["num_episodes"]
FRAME_SKIP_TRAINING = CONFIG["frame_skip_training"]
FRAME_SKIP_RECORDING = CONFIG["frame_skip_recording"]

# --- Training Parameters ---
GAMMA = CONFIG["gamma"]
EPSILON_START = CONFIG["epsilon_start"]
EPSILON_END = CONFIG["epsilon_end"]
EPSILON_DECAY = CONFIG["epsilon_decay"]
MODEL_SAVE_FREQ = CONFIG["model_save_freq"]
BATCH_SIZE = CONFIG["batch_size"]
MEMORY_CAPACITY = CONFIG["memory_capacity"]
LEARNING_RATE = CONFIG["learning_rate"]
TAU = CONFIG["tau"]
N_STEP = CONFIG["n_step"]
GRAD_CLIP_NORM = CONFIG["grad_clip_norm"]
EPSILON_DECAY_RATE_STEP = CONFIG["epsilon_decay_rate_step"]
USE_BEST_MODEL_CALLBACK = CONFIG["use_best_model_callback"]
BEST_MODEL_SMOOTHING_WINDOW = CONFIG["best_model_smoothing_window"]
VALIDATION_EPISODES = CONFIG["validation_episodes"]

# --- Setup Google Drive ---
def setup_google_drive(logger):
    drive_mounted = os.path.exists('/content/drive/My Drive')
    if not drive_mounted:
        try:
            from google.colab import drive
            drive.mount('/content/drive')
            logger.info("Google Drive mounted successfully.")
        except Exception as e:
            logger.error(f"Error mounting Google Drive: {e}")
            return False

    os.makedirs(VIDEO_DIR, exist_ok=True)
    os.makedirs(DRIVE_MODEL_DIR, exist_ok=True)
    os.makedirs(DRIVE_WAD_DIR, exist_ok=True)
    os.makedirs(LOCAL_WAD_DIR, exist_ok=True)
    logger.info("Google Drive directories created successfully.")
    return True

def copy_scenarios_to_drive(logger):
    local_scenarios_dir = "/usr/local/lib/python3.10/dist-packages/vizdoom/scenarios"
    wad_files = glob.glob(os.path.join(local_scenarios_dir, "*.wad"))
    for wad_file in wad_files:
        dest_file = os.path.join(DRIVE_WAD_DIR, os.path.basename(wad_file))
        if os.path.exists(dest_file):
            choice = input(f"File {os.path.basename(dest_file)} exists in Drive. Replace? (yes/no): ").lower()
            if choice == "yes":
                try:
                    shutil.copy(wad_file, dest_file)
                    logger.info(f"Replaced: {os.path.basename(wad_file)}")
                except Exception as e:
                    logger.error(f"Error replacing: {os.path.basename(wad_file)}, error: {e}")
            else:
                logger.info(f"Skipped: {os.path.basename(wad_file)}")
        else:
            try:
                shutil.copy(wad_file, dest_file)
                logger.info(f"Copied: {os.path.basename(wad_file)}")
            except Exception as e:
                logger.error(f"Error copying: {os.path.basename(wad_file)}, error: {e}")
    return True

# --- User Input for Recording ---
def get_record_choices(logger):
    record_choice = input("Do you want to record the game? (yes/no): ").lower()
    if record_choice == "yes":
        record_lmp_choice = input("Record .lmp demo files? (yes/no): ").lower()
        RECORD_LMP = record_lmp_choice == "yes"
        record_video_choice = input("Record video (MP4)? (yes/no): ").lower()
        RECORD_VIDEO = record_video_choice == "yes"
    else:
        RECORD_LMP = False
        RECORD_VIDEO = False
    logger.info(f"Record LMP: {RECORD_LMP}, Record Video: {RECORD_VIDEO}")
    return RECORD_LMP, RECORD_VIDEO

# --- User Input for Training Episodes ---
def get_num_episodes(logger):
    num_episodes_choice = input("Enter the number of episodes to run: ")
    try:
        NUM_EPISODES = int(num_episodes_choice)
    except ValueError:
        logger.warning("Invalid input for number of episodes. Using default (10).")
        NUM_EPISODES = 10
    logger.info(f"Number of Episodes: {NUM_EPISODES}")
    return NUM_EPISODES

# --- User Input for FRAME_SKIP ---
def get_frame_skips(logger):
    FRAME_SKIP_TRAINING = int(input("Enter FRAME_SKIP value for training (default 4): ") or "4")
    FRAME_SKIP_RECORDING = int(input("Enter FRAME_SKIP value for recording (default 1): ") or "1")
    FRAME_SKIP = FRAME_SKIP_TRAINING if not RECORD_VIDEO else FRAME_SKIP_RECORDING
    logger.info(f"Frame Skip Training: {FRAME_SKIP_TRAINING}, Frame Skip Recording: {FRAME_SKIP_RECORDING}")
    return FRAME_SKIP_TRAINING, FRAME_SKIP_RECORDING, FRAME_SKIP

def get_wad_choice(logger):
    print("Choose a WAD file to use:")
    print("1. Use current scenario WAD (defend_the_center.cfg with vizdoom assets)")
    print("2. Use original Doom WAD from Google Drive (ViZDoomWADs folder with defend_the_center.cfg)")
    wad_choice = input("Enter your choice (1 or 2): ")
    logger.info(f"WAD Choice: {wad_choice}")
    return wad_choice

def setup_scenario(wad_choice, logger):
    global SCENARIO_NAME  # Ensure SCENARIO_NAME is treated as a global variable
    SCENARIO_NAME = "defend_the_center.cfg"  # Set a default value

    if wad_choice == "1":
        try:
            import vizdoom
            vizdoom_path = os.path.dirname(vizdoom.__file__)
            SCENARIO_PATH = os.path.join(vizdoom_path, "scenarios", SCENARIO_NAME)
            wad_path = None
            logger.info(f"Using default scenario path: {SCENARIO_PATH}")
            return SCENARIO_PATH, wad_path
        except Exception as e:
            logger.error(f"Error loading default scenario: {e}")
            # Fallback to a default scenario if an error occurs
            SCENARIO_PATH = os.path.join(vizdoom_path, "scenarios", SCENARIO_NAME)
            return SCENARIO_PATH, None

    elif wad_choice == "2":
        wad_files = glob.glob(os.path.join(DRIVE_WAD_DIR, "*.wad"))
        if not wad_files:
            logger.warning("No WAD files found in ViZDoomWADs. Using default scenario.")
            return setup_scenario("1", logger)
        print("Available WAD files in ViZDoomWADs:")
        for i, file in enumerate(wad_files):
            print(f"{i+1}. {os.path.basename(file)}")
        wad_file_choice = input("Enter the number of the WAD file to use: ")
        try:
            wad_file_choice = int(wad_file_choice)
            if 1 <= wad_file_choice <= len(wad_files):
                wad_path = wad_files[wad_file_choice - 1]
                local_wad_path = os.path.join(LOCAL_WAD_DIR, os.path.basename(wad_path))
                if not os.path.exists(local_wad_path):
                    print(f"Copying {os.path.basename(wad_path)} from Google Drive to local...")
                    try:
                        shutil.copy(wad_path, local_wad_path)
                        logger.info(f"Copied {os.path.basename(wad_path)} from Google Drive to local.")
                    except Exception as e:
                        logger.error(f"Error copying wad file to local: {e}")
                        return setup_scenario("1", logger)  # Fallback to default scenario
                else:
                    print(f"{os.path.basename(wad_path)} already exists locally. Using local copy.")
                    logger.info(f"{os.path.basename(wad_path)} already exists locally.")
                wad_path = local_wad_path
                # SCENARIO_NAME is already set to "defend_the_center.cfg" at the beginning of the function
                import vizdoom
                vizdoom_path = os.path.dirname(vizdoom.__file__)
                SCENARIO_PATH = os.path.join(vizdoom_path, "scenarios", SCENARIO_NAME)
                logger.info(f"Using WAD file: {wad_path}, scenario path: {SCENARIO_PATH}")
                return SCENARIO_PATH, wad_path
            else:
                logger.warning("Invalid WAD file choice. Using default scenario.")
                return setup_scenario("1", logger)
        except ValueError:
            logger.warning("Invalid input. Using default scenario.")
            return setup_scenario("1", logger)
    else:
        logger.warning("Invalid choice. Using default scenario.")
        return setup_scenario("1", logger)

# --- DQN ---
class DQN(nn.Module):
    def __init__(self, input_shape, num_actions):
        super(DQN, self).__init__()
        self.conv1 = nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)
        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)
        self.fc1 = nn.Linear(self.get_conv_output(input_shape), 512)
        self.fc2 = nn.Linear(512, num_actions)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = x.view(x.size(0), -1)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return x

    def get_conv_output(self, shape):
        o = self.conv1(torch.zeros(1, *shape))
        o = self.conv2(o)
        o = self.conv3(o)
        return int(np.prod(o.size()))

# --- Prioritized Experience Replay Memory ---
class PrioritizedReplayMemory:
    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):
        self.capacity = capacity
        self.memory = []
        self.position = 0
        self.priorities = np.zeros(capacity, dtype=np.float32)
        self.alpha = alpha
        self.beta_start = beta_start
        self.beta_frames = beta_frames
        self.frame = 0

    def push(self, state, action, reward, next_state, done):
        max_priority = self.priorities.max() if self.memory else 1.0

        if len(self.memory) < self.capacity:
            self.memory.append(None)

        self.memory[self.position] = (state, action, reward, next_state, done)
        self.priorities[self.position] = max_priority
        self.position = (self.position + 1) % self.capacity

    def sample(self, batch_size):
        priorities = self.priorities[:len(self.memory)]
        probabilities = priorities**self.alpha
        probabilities /= probabilities.sum()

        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)
        samples = [self.memory[idx] for idx in indices]

        # Compute IS weights
        beta = min(1.0, self.beta_start + (self.frame * (1 - self.beta_start) / self.beta_frames))
        weights = (len(self.memory) * probabilities[indices])**(-beta)
        weights /= weights.max()
        self.frame += 1

        return samples, indices, weights

    def update_priorities(self, indices, td_errors):
        self.priorities[indices] = np.abs(td_errors) + 1e-6

    def __len__(self):
        return len(self.memory)

# --- N-step Buffer ---
class NStepBuffer:
    def __init__(self, n_step):
        self.n_step = n_step
        self.buffer = []

    def push(self, state, action, reward, next_state, done):
        self.buffer.append((state, action, reward, next_state, done))

    def get(self):
        if len(self.buffer) < self.n_step:
            return None

        n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done = self.buffer[0]
        for i in range(1, self.n_step):
            state, action, reward, next_state, done = self.buffer[i]
            n_step_reward += reward * GAMMA**i
            n_step_next_state = next_state
            n_step_done = done or n_step_done

        self.buffer = self.buffer[1:]
        return n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done

    def __len__(self):
        return len(self.buffer)

# --- DQNAgent ---
class DQNAgent:
    def __init__(self, state_shape, num_actions, learning_rate, gamma, epsilon_start, epsilon_end, epsilon_decay, memory_capacity, batch_size, tau, n_step, grad_clip_norm, epsilon_decay_rate_step):
        self.state_shape = state_shape
        self.num_actions = num_actions
        self.gamma = gamma
        self.epsilon = epsilon_start  # Set initial epsilon
        self.epsilon_start = epsilon_start
        self.epsilon_end = epsilon_end
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.tau = tau
        self.grad_clip_norm = grad_clip_norm
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.policy_net = DQN(state_shape, num_actions).to(self.device)
        self.target_net = DQN(state_shape, num_actions).to(self.device)
        self.target_net.load_state_dict(self.policy_net.state_dict())
        self.target_net.eval()

        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)
        self.memory = PrioritizedReplayMemory(memory_capacity)
        self.n_step_buffer = NStepBuffer(n_step)
        self.epsilon_decay_rate_step = epsilon_decay_rate_step
        self.training_step = 0
        self.best_avg_reward = float('-inf')
        self.best_model_state = None

    def select_action(self, state):
        if np.random.rand() <= self.epsilon:
            return np.random.choice(self.num_actions)
        else:
            with torch.no_grad():
                state = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
                q_values = self.policy_net(state)
                return q_values.argmax(dim=1).item()

    def update_epsilon(self):
        self.epsilon = max(self.epsilon_end, self.epsilon_start * (self.epsilon_decay ** (self.training_step / self.epsilon_decay_rate_step)))

    def learn(self):
        if len(self.memory) < self.batch_size:
            return None, None, None
        self.training_step += 1
        samples, indices, weights = self.memory.sample(self.batch_size)
        batch = tuple(zip(*samples))

        state_batch = torch.tensor(np.array(batch[0]), dtype=torch.float32, device=self.device)
        action_batch = torch.tensor(batch[1], dtype=torch.long, device=self.device).unsqueeze(1)
        reward_batch = torch.tensor(batch[2], dtype=torch.float32, device=self.device).unsqueeze(1)
        next_state_batch = torch.tensor(np.array(batch[3]), dtype=torch.float32, device=self.device)
        done_batch = torch.tensor(batch[4], dtype=torch.float32, device=self.device).unsqueeze(1)
        weights = torch.tensor(weights, dtype=torch.float32, device=self.device).unsqueeze(1)

        q_values = self.policy_net(state_batch).gather(1, action_batch)

        with torch.no_grad():
            next_q_values_online = self.policy_net(next_state_batch)
            next_actions = next_q_values_online.argmax(dim=1).unsqueeze(1)
            next_q_values_target = self.target_net(next_state_batch).gather(1, next_actions)
            expected_q_values = reward_batch + self.gamma * next_q_values_target * (1 - done_batch)

        td_errors = expected_q_values - q_values
        loss = (weights * nn.MSELoss(reduction='none')(q_values, expected_q_values)).mean()

        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.grad_clip_norm)
        self.optimizer.step()

        self.memory.update_priorities(indices, td_errors.detach().cpu().numpy().squeeze())
        return loss.item(), q_values.mean().item(), torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), self.grad_clip_norm).item()

    def update_target_network(self):
        for target_param, local_param in zip(self.target_net.parameters(), self.policy_net.parameters()):
            target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)

    def save_model(self, model_path):
        torch.save({
            'policy_net_state_dict': self.policy_net.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
        }, model_path)

    def load_model(self, model_path):
        checkpoint = torch.load(model_path, map_location=self.device)
        self.policy_net.load_state_dict(checkpoint['policy_net_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.target_net.load_state_dict(self.policy_net.state_dict())

    def best_model_callback(self, episode_rewards, logger, model_dir, best_model_smoothing_window):
        if len(episode_rewards) >= best_model_smoothing_window:
            avg_reward = np.mean(episode_rewards[-best_model_smoothing_window:])
        else:
            avg_reward = np.mean(episode_rewards) if episode_rewards else float('-inf')
        if avg_reward > self.best_avg_reward:
            self.best_avg_reward = avg_reward
            self.best_model_state = deepcopy(self.policy_net.state_dict())
            model_filename = f"best_dqn_model.pth"
            model_path = os.path.join(model_dir, model_filename)
            self.save_model(model_path)
            logger.info(f"Best model saved to {model_path} with reward: {self.best_avg_reward}")
        return self

def scan_for_models(agent, model_dir, logger):
    model_files = glob.glob(os.path.join(model_dir, "*.pth"))

    if model_files:
        print("Available models:")
        for i, file in enumerate(model_files):
            print(f"{i+1}. {os.path.basename(file)}")
        print(f"{len(model_files)+1}. Create new model")

        choice = input("Enter your choice: ")
        try:
            choice = int(choice)
            if choice in range(1, len(model_files) + 2):
                if choice <= len(model_files):
                    model_path = model_files[choice - 1]
                    try:
                        agent.load_model(model_path)
                        logger.info(f"Model loaded from {model_path}")
                    except Exception as e:
                        logger.error(f"Error loading model: {e}")
                        # Return new Agent on error, using CONFIG values:
                        return DQNAgent(agent.state_shape, agent.num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP, GRAD_CLIP_NORM, EPSILON_DECAY_RATE_STEP)
                else:
                    logger.info("Creating new model...")
                    # Create new agent, using CONFIG values:
                    return DQNAgent(agent.state_shape, agent.num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP, GRAD_CLIP_NORM, EPSILON_DECAY_RATE_STEP)
            else:
                logger.warning("Invalid choice. Creating new model.")
                # Return new Agent on invalid choice, using CONFIG values:
                return DQNAgent(agent.state_shape, agent.num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP, GRAD_CLIP_NORM, EPSILON_DECAY_RATE_STEP)
        except ValueError:
            logger.warning("Invalid input. Creating new model.")
            # Return new Agent on invalid input, using CONFIG values:
            return DQNAgent(agent.state_shape, agent.num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP, GRAD_CLIP_NORM, EPSILON_DECAY_RATE_STEP)
    else:
        logger.info("No existing models found. Creating new model...")
        # Return new Agent on no models found, using CONFIG values:
        return DQNAgent(agent.state_shape, agent.num_actions, LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP, GRAD_CLIP_NORM, EPSILON_DECAY_RATE_STEP)
    return agent

def setup_vizdoom(scenario_path, wad_path=None, logger=None):
    try:
        from vizdoom import DoomGame, ScreenFormat, ScreenResolution, Mode
        game = DoomGame()

        if scenario_path:
            game.load_config(scenario_path)
        else:
            raise ValueError("Scenario path is not defined.")

        if wad_path:
            game.set_doom_game_path(wad_path)
        game.set_window_visible(False)
        game.set_screen_format(ScreenFormat.RGB24)
        game.set_screen_resolution(ScreenResolution.RES_320X240)

        if RECORD_LMP:
            game.set_mode(Mode.PLAYER)
            os.makedirs(LMP_DIR, exist_ok=True)

        game.init()
        num_actions = game.get_available_buttons_size()
        actions = np.identity(num_actions, dtype=int).tolist()

        screen_height, screen_width = game.get_screen_height(), game.get_screen_width()
        channels = game.get_screen_channels()
        state_shape = (STACK_SIZE * channels, 84, 84)
        logger.info("ViZDoom initialized successfully.")
        return game, actions, state_shape
    except Exception as e:
        if logger:
            logger.error(f"Error during ViZDoom setup: {e}")
        print(f"Error during ViZDoom setup: {e}")
        return None, None, None

# --- Video Writer ---
def setup_video_writer(video_path, video_fps, logger=None):
    if RECORD_VIDEO:
      try:
        video_writer = imageio.get_writer(video_path, fps=video_fps)
        logger.info(f"Video Writer set up at: {video_path}")
        return video_writer
      except Exception as e:
          logger.error(f"Error during video writer setup: {e}")
          return None
    else:
      return None

# --- TensorBoard Setup ---
def setup_tensorboard(log_dir, logger=None):
    now = datetime.now()
    dt_string = now.strftime("%Y%m%d%H%M")
    log_dir = os.path.join(log_dir, f"{dt_string}_experiment")
    if logger:
      logger.info(f"TensorBoard logging to: {log_dir}")
    return SummaryWriter(log_dir=log_dir)

# --- Frame Stacking ---
def preprocess_frame(frames):
    if len(frames) == 0:
        return []
    processed_frames = []
    for frame in frames:
        if len(frame.shape) == 3 and frame.shape[0] == 3:
            frame = np.mean(frame, axis=0)
        frame = frame.astype(np.float32) / 255.0
        frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)
        if len(frame.shape) == 3:
            frame = frame.transpose(2, 0, 1)
        else:
            frame = np.expand_dims(frame, axis=0)
        processed_frames.append(frame)
    return processed_frames

def create_stacked_state(state_buffer):
    processed_frames = preprocess_frame(state_buffer)
    if any(f is None for f in processed_frames) or not all(f.shape == processed_frames[0].shape for f in processed_frames):
        print("Error: Inconsistent or None frames in processed_frames")
        return None
    stacked_state = np.concatenate(processed_frames, axis=0)
    return stacked_state

def get_game_state_info(game):
    game_state = game.get_state()
    if game_state is not None:
        damage_taken = game_state.game_variables[0]
        damage_inflicted = game_state.game_variables[1]
        return damage_taken, damage_inflicted
    else:
        return 0, 0

def run_episode(agent, game, actions, episode, frame_skip, lmp_dir, record_video, video_writer, stack_size, state_shape, logger, episode_metrics_df):
    if RECORD_LMP:
        lmp_file_path = os.path.join(lmp_dir, f"episode_{episode + 1}.lmp")
        game.new_episode(lmp_file_path)
    else:
        game.new_episode()

    game_state = game.get_state()
    if game_state is None or game_state.screen_buffer is None:
        logger.error(f"Skipping episode {episode+1} due to invalid game state.")
        return None, 0, 0, 0, 0, 0, None

    state_buffer = [game_state.screen_buffer] * stack_size
    state = create_stacked_state(state_buffer)
    if state is None:
        logger.error(f"Skipping episode {episode + 1} due to state initialization error.")
        return None, 0, 0, 0, 0, 0, None

    total_reward = 0
    step_count = 0
    episode_start_time = time.time()
    damage_taken = 0
    damage_inflicted = 0
    action_counts = Counter()
    inference_times = []
    loss, avg_q_value, grad_norm = None, None, None

    while not game.is_episode_finished():
        inference_start_time = time.time()

        # Assign a value to action_index before using it
        action_index = agent.select_action(state)

        inference_end_time = time.time()
        inference_times.append(inference_end_time - inference_start_time)

        action = actions[action_index]
        action_counts[action_index] += 1

        reward = game.make_action(action, frame_skip)
        done = game.is_episode_finished()

        if not done:
            game_state = game.get_state()
            if game_state and game_state.screen_buffer is not None:
                next_frame = game_state.screen_buffer
                state_buffer.pop(0)
                state_buffer.append(next_frame)
                next_state = create_stacked_state(state_buffer)

                if next_state is None:
                    logger.error(f"Error: next_state is None in episode {episode + 1} at step {step_count + 1}.")
                    break
                damage_taken_step, damage_inflicted_step = get_game_state_info(game)
                damage_taken += damage_taken_step
                damage_inflicted += damage_inflicted_step
            else:
                logger.error(f"Error: Invalid state in episode {episode+1}, step {step_count+1}.")
                next_state = np.zeros(state_shape)
                break
        else:
            next_state = np.zeros(state_shape)

        agent.n_step_buffer.push(state, action_index, reward, next_state, done)

        n_step_experience = agent.n_step_buffer.get()
        if n_step_experience:
            n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done = n_step_experience
            agent.memory.push(n_step_state, n_step_action, n_step_reward, n_step_next_state, n_step_done)

        loss_step, avg_q_value_step, grad_norm_step = agent.learn()
        if loss_step is not None:
            loss = loss_step
            avg_q_value = avg_q_value_step
            grad_norm = grad_norm_step

        state = next_state
        total_reward += reward
        step_count += 1

        if record_video and not done:
            most_recent_frame = state_buffer[-1]
            video_writer.append_data(most_recent_frame)

    episode_survival_time = time.time() - episode_start_time
    total_actions = sum(action_counts.values())
    if total_actions > 0:
        action_diversity = sum(count / total_actions for count in action_counts.values()) / len(actions)
    else:
        action_diversity = 0

    avg_inference_time = np.mean(inference_times) if inference_times else 0

    if total_reward is not None:
        new_row = {'episode': episode + 1, 'reward': total_reward, 'steps': step_count,
                   'survival_time': episode_survival_time, 'damage_taken': damage_taken,
                    'damage_inflicted': damage_inflicted, 'action_diversity': action_diversity,
                    'avg_inference_time': avg_inference_time, 'loss': loss if loss is not None else np.nan,
                    'avg_q_value': avg_q_value if avg_q_value is not None else np.nan,
                    'grad_norm': grad_norm if grad_norm is not None else np.nan}

        return total_reward, step_count, episode_survival_time, damage_taken, damage_inflicted, action_diversity, avg_inference_time, new_row
    else:
        return total_reward, step_count, episode_survival_time, damage_taken, damage_inflicted, action_diversity, avg_inference_time, None

# --- Validation ---
def validate_model(agent, game, actions, state_shape, stack_size, num_episodes, logger):
    """Validates the model by running a number of episodes and calculating the mean reward."""
    total_rewards = []
    for episode in range(num_episodes):
        total_reward, _, _, _, _, _, _ , _ = run_episode(agent, game, actions, episode, 1, None, False, None, stack_size, state_shape, logger, pd.DataFrame())
        if total_reward is not None:
            total_rewards.append(total_reward)
    return np.mean(total_rewards) if total_rewards else float('-inf')

def best_model_callback(agent, episode_rewards, logger, model_dir, best_model_smoothing_window):
    """Callback to save the best model based on a smoothed average reward."""
    if len(episode_rewards) >= best_model_smoothing_window:
        avg_reward = np.mean(episode_rewards[-best_model_smoothing_window:])
    else:
        avg_reward = np.mean(episode_rewards) if episode_rewards else float('-inf')
    if avg_reward > agent.best_avg_reward:
        agent.best_avg_reward = avg_reward
        agent.best_model_state = deepcopy(agent.policy_net.state_dict())
        model_filename = f"best_dqn_model.pth"
        model_path = os.path.join(model_dir, model_filename)
        agent.save_model(model_path)
        logger.info(f"Best model saved to {model_path} with reward: {agent.best_avg_reward}")

# --- Save metrics to CSV ---
def save_metrics(episode_rewards, episode_lengths, episode_survival_times,
                 episode_damage_taken, episode_damage_inflicted, logger, episode_metrics_df, training_time,
                 ram_usage_list, gpu_memory_usage_list, action_diversity_list):  # Add parameters
    metrics_df = pd.DataFrame({
    'episode': range(1, len(episode_rewards) + 1),
    'reward': episode_rewards,
    'steps': episode_lengths,
    'epsilon': [EPSILON_START * (EPSILON_DECAY ** (i / EPSILON_DECAY_RATE_STEP)) for i in range(len(episode_rewards))],
    'training_time': training_time,
    'survival_time': episode_survival_times,
    'damage_taken': episode_damage_taken,
    'damage_inflicted': episode_damage_inflicted,
        'ram_usage': ram_usage_list,
        'gpu_memory_usage': gpu_memory_usage_list,
        'action_diversity': action_diversity_list,
    'model_save_freq': MODEL_SAVE_FREQ,
    'batch_size': BATCH_SIZE,
    'memory_capacity': MEMORY_CAPACITY,
    'learning_rate': LEARNING_RATE,
    'tau': TAU,
    'n_step': N_STEP
    })
    # Merge episode metrics with overall metrics
    metrics_df = pd.merge(metrics_df, episode_metrics_df, on='episode', how='left')

    now = datetime.now()
    dt_string = now.strftime("%Y%m%d%H%M")
    metrics_filename = f"{dt_string}_training_metrics.csv"
    metrics_path = os.path.join(DRIVE_MODEL_DIR, metrics_filename)
    metrics_df.to_csv(metrics_path, index=False)
    logger.info(f"Training metrics saved to {metrics_path}")

# --- Main Training Loop ---
def main():
    global training_start_time
    # --- Setup  ---
    TIMESTAMP = datetime.now().strftime("%Y%m%d%H%M")
    LOG_DIR = "logs"
    os.makedirs(LOG_DIR, exist_ok=True)  # Create logs directory
    logger = setup_logger(LOG_DIR, TIMESTAMP)

    setup_google_drive(logger)
    episode_metrics_df = pd.DataFrame(columns=['episode', 'reward', 'steps', 'survival_time',
                                               'damage_taken', 'damage_inflicted', 'action_diversity',
                                               'avg_inference_time', 'loss', 'avg_q_value',
                                               'grad_norm'])
    record_lmp, record_video = get_record_choices(logger)
    NUM_EPISODES = get_num_episodes(logger)
    FRAME_SKIP_TRAINING, FRAME_SKIP_RECORDING, FRAME_SKIP = get_frame_skips(logger)
    wad_choice = get_wad_choice(logger)
    SCENARIO_PATH, wad_path = setup_scenario(wad_choice, logger)
    game, actions, state_shape = setup_vizdoom(SCENARIO_PATH, wad_path, logger)
    video_writer = setup_video_writer(VIDEO_PATH, VIDEO_FPS, logger)
    tensorboard_writer = setup_tensorboard("runs", logger)

    # --- Agent Initialization ---
    agent = DQNAgent(state_shape, len(actions), LEARNING_RATE, GAMMA, EPSILON_START, EPSILON_END, EPSILON_DECAY, MEMORY_CAPACITY, BATCH_SIZE, TAU, N_STEP, GRAD_CLIP_NORM, EPSILON_DECAY_RATE_STEP)
    agent = scan_for_models(agent, DRIVE_MODEL_DIR, logger)
    # --- Setup Episode Metrics ---
    episode_rewards = []
    episode_lengths = []
    episode_damage_taken = []
    episode_damage_inflicted = []
    episode_survival_times = []
    inference_times = []
    ram_usage_list = []  # Add list to store RAM usage
    gpu_memory_usage_list = []  # Add list to store GPU memory usage
    action_diversity_list = [] # Add list to store action diversity
    training_start_time = time.time()
    best_avg_reward = float('-inf')
    # --- Training Loop ---
    for episode in range(NUM_EPISODES):
        (
            total_reward,
            step_count,
            episode_survival_time,
            damage_taken,
            damage_inflicted,
            action_diversity,
            avg_inference_time,
            new_row,
        ) = run_episode(
            agent,
            game,
            actions,
            episode,
            FRAME_SKIP,
            LMP_DIR,
            RECORD_VIDEO,
            video_writer,
            STACK_SIZE,
            state_shape,
            logger,
            episode_metrics_df,
        )
        if total_reward is not None:
            episode_metrics_df = episode_metrics_df._append(new_row, ignore_index=True)
            episode_survival_times.append(episode_survival_time)
            episode_rewards.append(total_reward)
            episode_lengths.append(step_count)
            episode_damage_taken.append(damage_taken)
            episode_damage_inflicted.append(damage_inflicted)
            agent.update_epsilon()

            if episode % MODEL_SAVE_FREQ == 0:
                dt_string = datetime.now().strftime("%Y%m%d%H%M")
                model_filename = f"{dt_string}_dqn_model_episode_{episode + 1}.pth"
                model_path = os.path.join(DRIVE_MODEL_DIR, model_filename)
                agent.save_model(model_path)
                logger.info(f"Model saved to {model_path} at episode {episode+1}")
            # --- Update Target Network ---
            agent.update_target_network()
            # --- Validation Step ---
            if USE_BEST_MODEL_CALLBACK:
                if (episode + 1) % BEST_MODEL_SMOOTHING_WINDOW == 0:
                    validation_reward = validate_model(agent, game, actions, state_shape, STACK_SIZE, VALIDATION_EPISODES, logger)
                    logger.info(f"Validation Reward (Episode {episode+1}): {validation_reward}")
                    best_model_callback(agent, episode_rewards, logger, DRIVE_MODEL_DIR, BEST_MODEL_SMOOTHING_WINDOW)
            # --- Log Metrics ---
            # --- Get memory usage ---
            process = psutil.Process(os.getpid())
            ram_usage = process.memory_info().rss / 1024**2
            gpu_memory_usage = torch.cuda.max_memory_allocated() / 1024**2 if torch.cuda.is_available() else 0

            ram_usage_list.append(ram_usage)  # Append to list
            gpu_memory_usage_list.append(gpu_memory_usage)  # Append to list
            action_diversity_list.append(action_diversity) # Append to list
            avg_reward = np.mean(episode_rewards[-100:])
            avg_survival_time = np.mean(episode_survival_times[-100:])
            avg_loss = np.mean(episode_metrics_df['loss'])
            avg_q_value = np.mean(episode_metrics_df['avg_q_value'])
            avg_grad_norm = np.mean(episode_metrics_df['grad_norm'])
            training_time = time.time() - training_start_time
            # --- Save metrics ---
            save_metrics(episode_rewards, episode_lengths, episode_survival_times,
                        episode_damage_taken, episode_damage_inflicted, logger, episode_metrics_df, training_time,
                        ram_usage_list, gpu_memory_usage_list, action_diversity_list)  # Pass the lists

            logger.info(
                f"Episode {episode+1}/{NUM_EPISODES}, Total Reward: {total_reward}, Steps: {step_count}, "
                f"Epsilon: {agent.epsilon:.3f}, Avg Reward (last 100): {avg_reward:.2f}, "
                f"Survival Time: {episode_survival_time:.2f}s, Avg Survival Time (last 100): {avg_survival_time:.2f}s, "
                f"Damage Taken: {damage_taken}, Damage Inflicted: {damage_inflicted}, Avg Inference Time: {avg_inference_time:.4f}s, "
                f"RAM Usage: {ram_usage:.2f}MB, GPU Mem Usage: {gpu_memory_usage:.2f}MB, Action Diversity: {action_diversity:.2f}"
                f"Training Time: {training_time:.2f}s, Avg Loss: {avg_loss:.4f}, Avg Q-Value: {avg_q_value:.4f}, Avg Grad Norm: {avg_grad_norm:.4f}"
            )

            tensorboard_writer.add_scalar("Reward/Total", total_reward, episode)
            tensorboard_writer.add_scalar("Reward/Average", avg_reward, episode)
            tensorboard_writer.add_scalar("Epsilon", agent.epsilon, episode)
            tensorboard_writer.add_scalar("Survival Time/Average", avg_survival_time, episode)
            tensorboard_writer.add_scalar("Action Diversity", action_diversity, episode)
            tensorboard_writer.add_scalar("Damage Taken", damage_taken, episode)
            tensorboard_writer.add_scalar("Damage Inflicted", damage_inflicted, episode)
            tensorboard_writer.add_scalar("RAM Usage", ram_usage, episode)
            tensorboard_writer.add_scalar("GPU Usage", gpu_memory_usage, episode)
            tensorboard_writer.add_scalar("Avg Inference Time", avg_inference_time, episode)
            tensorboard_writer.add_scalar("Loss/Average", avg_loss, episode)
            tensorboard_writer.add_scalar("Q-Value/Average", avg_q_value, episode)
            tensorboard_writer.add_scalar("Gradient Norm/Average", avg_grad_norm, episode)

            # --- Clear memory ---
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

                # --- Save Best Model ---
                if USE_BEST_MODEL_CALLBACK:
                    best_model_path = os.path.join(DRIVE_MODEL_DIR, "best_dqn_model.pth")
                    if os.path.exists(best_model_path):
                        agent.load_model(best_model_path)
                        logger.info("Loaded best model.")
                    else:
                        logger.warning("Best model file not found. Skipping loading.")

    # --- Close the writer ---
    if RECORD_VIDEO:
        video_writer.close()
    tensorboard_writer.close()

    game.close()
    print("Done!")

# --- Run the main function ---
if __name__ == "__main__":
    main()
